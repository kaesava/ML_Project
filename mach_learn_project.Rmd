Project Submission for Coursera "Practical Machine Learning" Course Project
------------------------
Author: Kesh K

##Overview and Data Source

This report is the project submission for Coursera "Practical Machine
Learning" Course Project.

We use data from accelerometers on the belt, forearm, arm, and dumbell of 
6 participants to build a model to determine if the barbell lifts were 
being performed correctly or incorrectly.

The data for this project come from this source: 
http://groupware.les.inf.puc-rio.br/har.

The authors have very kindly made the data available for this kind of
assignment.

The training and testing datasets were downloaded on 16-Jul-2015 at 11:45AM
Australian Eastern Standard time AEST from the links below:

training: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

testing: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

##Environment Setup and Data Load

First we loaded R libraries that we will use. We set the seed for 
reproducibility and take advantage of available parallel processing using
the "doParallel" library and setting the number of threads.

We then read the training and testing csv files that we downloaded into
datasets. Exploring the data, it is clear that missing values are coded as
"NA". However there are also values coded as "#DIV/0!", which presumably is
an error from a spreadsheeting program like Excel. We mark these as NA too
since we do not know what the true value might be.

```{r echo=TRUE, warning=FALSE, error=FALSE, message=FALSE}
require(ggplot2)
require(caret)
require(doParallel)

set.seed(3433)

registerDoParallel(cores=2)

training <- read.table("pml-training.csv", header=TRUE, sep=",", 
    na.strings=c("NA", "#DIV/0!"), comment.char="",nrows = 19624, quote="\"")
testing <- read.table("pml-testing.csv", header=TRUE, sep=",",
    na.strings=c("NA", "#DIV/0!"), comment.char="",nrows = 21, quote="\"")
```

##Data Cleansing - Missing Values and extraneous predictors

We investigate the training dataset for missing values.

```{r echo=TRUE, cache=FALSE, warning=FALSE, error=FALSE, message=FALSE}
fields_with_missing_vals <- sapply(training, function(x) sum(is.na(x)))
table(fields_with_missing_vals)
```

There are 60 fields with no missing values, and 100 with 19,216 or more 
missing values! Given we only have 19,622 records in the training data set,
including these fields with over ~98% missing values will skew the results.
We exclude them from the analysis.

Further: Looking at the data, it seems like most columns  with missing values
are functions (like max, min, skewness, etc.) of existing raw data columns.

```{r echo=TRUE, cache=FALSE, warning=FALSE, error=FALSE, message=FALSE}
clean_training <- training[, fields_with_missing_vals == 0]
```

Also, the first field (serial #) and user name qualify each record and for 
the purpose of this exercise, we exclude them in the analysis as they will
add little predictive value. The time-stamp and window fields are also 
markers for time-frame related data and while a full time-series analysis
might include them, we exclude them to focus on the time-independent impact of
the measurement variables on the classe variable. Incidentally, the test
dataset has these values missing.

```{r echo=TRUE, cache=FALSE, warning=FALSE, error=FALSE, message=FALSE}
clean_training <- clean_training[, -c(1:7)]
```

##Principal Components Analysis

We then run a Principal components analysis (pca) to determine variables that
have the most impact (account for most discrimination/variance) on the classe
variable. We use a variance threshold of 85%. This significantly reduces
the domain space while largely keeping the impactful variables. We remove the
last (classe) variable when building the pre-processing (pca) model.

```{r echo=TRUE, cache=FALSE, warning=FALSE, error=FALSE, message=FALSE}
prePModel <- preProcess(clean_training[,-53], method="pca", thresh = 0.85)
clean_pca_training <- predict(prePModel, clean_training[,-53])
```

The dataset now has `r dim(clean_pca_training)[2]` columns/predictors instead
of `r dim(clean_training[,-53])[2]` predictors.

##Building a Training Model

Since we do not have separate testing and validation data sets, we choose 
to use 10-fold cross-validation, which is quite commonly used to ensure
that the model is not over-fit to the training set; i.e., the out of sample
error rate is more realistic (less optimistic). Effectively, the training
set is partitioned 10 times and one partition is used as the testing 
(validation) dataset and the others as training, and this is repeated for
each partition; the best aggregate model is selected.

We elected to use the Random Forest training algorithm as they known for
their high accuracy while not over-fitting to the training dataset, 
especially when used in conjunction with cross-validation, which is what we
have done.

```{r echo=TRUE, cache=TRUE, warning=FALSE, error=FALSE, message=FALSE}
trc <- trainControl(method = "cv", number = 10)
model <- train(clean_pca_training, clean_training$classe, 
               method="rf", trControl=trc)
```

##Out-of-sample Error

We expect the out-of-sample error % to be:

```{r echo=TRUE, cache=FALSE, warning=FALSE, error=FALSE, message=FALSE}
sprintf("%1.2f%%", 100 - round(max(model$results$Accuracy)*100,1))
```
or typically larger. This is the percentage of mis-classified records 
(1 minus the accuracy).

##Preparing the Testing Dataset

We now prepare the testing dataset in the same way we prepared the training
dataset. We remove missing value fields (only the same ones we removed in
the training set) and the first 7 columns (again, the same ones we removed
in the training set). We apply the same Principal Components model that we
applied in the training set to the testing set (here we remove the last 
column which is the problem_id).

```{r echo=TRUE, cache=FALSE, warning=FALSE, error=FALSE, message=FALSE}
clean_testing <- testing[, fields_with_missing_vals == 0]
clean_testing <- clean_testing[, -c(1:7)]
clean_pca_testing <- predict(prePModel, clean_testing[,-53])
```

##Predicting Test data

We then predict using the model on the cleansed (PCA-applied) testing dataset
and save the data as a character vector for outputting in a format friendly
for submission.

```{r echo=TRUE, cache=FALSE, warning=FALSE, error=FALSE, message=FALSE}
answers <- as.character(predict(model, clean_pca_testing))
```

##Saving predictions

We define a pml_write file to output answers in a format that can be used
to upload to get test results and run it to produce the output files.

```{r echo=TRUE, cache=FALSE, warning=FALSE, error=FALSE, message=FALSE}
pml_write_files = function(x) {
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i], file=filename, quote=FALSE, row.names=FALSE, 
            col.names=FALSE)
    }
}
pml_write_files(answers)
```

##Conclusion
In this report, we explored the training dataset and pre-processed it
by removing extraneous predictors via missing data analysis, principal 
components for the outcome "classe" variable. We then fit a random forest
algorithm model using 10-fold cross validation and predicted test values.
